# Exponential-Decay-for-Ordering-Time-Series-MLPRegression

Multi-Layer Perceptrons, if utilized to work on time series data or sequential data, need to have the inputs fed to the first input layer in such a way as to describe the order that they were initially in. The way that I have designed it here is to apply an exponential decay function to the inputs so that the inputs that are closer in time or sequence to the target variable are given a higher weighting compared to the inputs that occur further in time or sequence from the target variable. 

In the function "order_via_decay" there is a requirement for the entire input sequence in the order that it occurs naturally, a hyperparameter requirement for the size of the batches to be fed to the MLP Regression (i.e., an input seqeuence of 5 will have 5 historical inputs for every 1 current input in a rolling fashion along the entire length of the sequence), and a hyperparameter requirement for the speed of exponential decay (values given should be 0<x<1, with values closer to 1 decaying slower than values closer to zero). 

In the colab sheet provided, I test the ability of the MLP Regressor to learn stock price values with and without the decay function. The end results were that utilizing the MLP Regressor with exponential decay to create order within the input sequence resulted in an average percentage difference between prediction and actual stock value of 3.71% and a standard deviation of 0.85 after a total of 270 runs. When utilizing the MLP Regressor without exponential decay the average percentage difference between prediction and actual stock value was 4.13% and a standard deviation of 1.65 after a total of 270 runs. These runs were done all using a MLP Regression architecture of 3 hidden layers composing 100, 25 and 10 neurons respectively. Other architectures of the MLP may yield better results but it seems that MLP Regression with imposed order from exponential decay is able to perform better on time series data than no order imposed. 
